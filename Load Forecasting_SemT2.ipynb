{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Herman Carstens EES732 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial on the basics of one method of load forecasting (linear regression) prepared for EES732 (Energy Management), and written in Jupyter (IPython Notebook). In order to play around with the code, download this notebook from the GitHub repository. The link should be top right. You will also need to install Python to edit and run the code. The easiest is to install it through [Anaconda](https://www.continuum.io/downloads). You will also need to install some of the additional libraries like `Pandas`, `Seaborn`, etc. The easiest way to do this, is to go the the Anaconda Command Prompt, and type in `conda install pandas` or whatever you need. Note that this is not necessary for reading this notebook, or seeing the graphs. Just for playing around with the code.\n",
    "\n",
    "This notebook discusses some basic theory needed for load forecasting, as well as three examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to set up Python and import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "#Displays graphs automatically\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from pandas import set_option\n",
    "set_option(\"display.max_rows\",16,\"display.max_columns\",13)\n",
    "import matplotlib.pyplot as plt         \n",
    "pd.options.display.mpl_style='default'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']=18,8 #figure sizes\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of this tutorial is based on [*Assessment of Automated Measurement and Verification (M&V) Methods* (2015) by Granderson et al., Lawrence Berkeley National Laboratory (LBNL-187225)](http://eetd.lbl.gov/publications/assessment-of-automated-measurement-a). Please cite the paper rather than this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a load forecasting model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A mathematical method or set of equations that can predict future energy use.\n",
    "\n",
    "Methods can be based on various considerations: Time of day, energy use at the previous time interval, occupancy, temperature (called covariates), the energy use of a similar day, etc.\n",
    "\n",
    "The mathematical model will give you a load profile (forecast), or energy uses at different times of the day, or the total energy use for a day or a week or a month.\n",
    "\n",
    "Note that 'forecast' refers to future data that we don't have. 'Predict' refers to the model's fit of the data we do have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load forecasting metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a \"good\" model?\n",
    "\n",
    "A \"good\" model is one that forecasts accurately according to certain metrics:\n",
    "* Total Error\n",
    "* Mean Error\n",
    "* Root Mean Square Error\n",
    "* Total Bias\n",
    "* Coefficient of Determination ($R^{2}$)\n",
    "* **Coefficient of Variance on the Root Mean Square Error: CV(RMSE)**\n",
    "* **Normalised Mean Bias Error**\n",
    "\n",
    "The last two methods are widely accepted the most useful ones, and complement each other well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CV(RMSE) and NMBE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**CV(RMSE):** A measure of well the model fits the data.\n",
    "\n",
    "**NMBE:** An expression for the actual forecast error, in percentage terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coefficient of Variance on the Root Mean Square Error - CV(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let:\n",
    "* $N$ be the number of data points in the evaluation (post-retrofit) period\n",
    "* $y_{i}$ be the energy measured at the $i^{th}$ time interval\n",
    "* $\\hat{y_{i}}$ (pronounced \"y hat\") be the energy predicted at the $i^{th}$ time interval\n",
    "* $\\bar{y}$ (\"pronounced \"y bar\") be the mean energy use over the evaluation period\n",
    "\n",
    "$CV(RMSE)=\\frac{\\sqrt{\\frac{1}{N}\\sum_{i}^{N}(y_{i}-\\hat{y}_{i})^{2}}}{\\bar{y}}\\times 100$\n",
    "\n",
    "Logic:\n",
    "* Find the differences between forecast and actual energy use at each data point: the Errors **(E)** (or bias)\n",
    "* Square these differences: the Squared Errors **(SE)**\n",
    "* Sum them, and divide them by the number of data points / measurements to find the mean: the Mean Square Error **(MSE)**\n",
    "* Find the square root: the Root Mean Square Error **(RMSE)**\n",
    "* Normalise by dividing by the average energy use during the period: the RMSE with respect to the actual mean **CV(RMSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Normalised Mean Bias Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$NMBE = \\frac{\\frac{1}{N}\\sum_{i}^{N}(y_{i}-\\hat{y}_{i})}{\\bar{y}}\\times 100$\n",
    "\n",
    "Logic:\n",
    "* Find the differences between the forecast and actual values: the Bias Error **(BE)**\n",
    "* Sum them, divide the the number of points: Mean Error **(MBE)**\n",
    "* Normalise by dividing by the mean energy use: Normalised Mean Bias Error **(NMBE)**\n",
    "\n",
    "**Digression**: Why not call this the CV(MBE), or call the CV(RMSE) the NMRSE? Why do we call it normalisation in the one case and CV in the other?\n",
    "\n",
    "Traditionally, $CV=\\frac{\\sigma}{\\mu}=\\frac{STDEV}{MEAN}$. It's like the normalised standard deviation.\n",
    "The standard deviation is calculated as a *squared error*. So in some ways, CV(RMSE) is a bit like the standard deviation of your *model* w.r.t. the *observed data*, divided by the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Forecasting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So now that we know what a good model would look how, how do we build them?\n",
    "\n",
    "Dozens of methods are available:\n",
    "* ARIMA (Autoregressive Integrated Moving Average). A Time Series Forecasting Method\n",
    "* Calibrated Simulation\n",
    "* ANN (Artificial Neural Networks)\n",
    "* Fourier Series Models\n",
    "* PCA (Principal Component Analysis) Regression\n",
    "* SVM (Support Vector Machines)\n",
    "* Linear Regression\n",
    "* Nearest Neighbour\n",
    "\n",
    "We will focus on *Linear Regression*: Good value for money. Simple and relatively accurate, though not the best. For the shootout and results, see [Granderson et al](http://eetd.lbl.gov/publications/assessment-of-automated-measurement-a).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Easy! \n",
    "\n",
    "**$Y=X \\beta + \\epsilon$**\n",
    "\n",
    "But this is matrix notation, so actually:\n",
    "\n",
    "$Y=\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\beta_{3}x_{3}+\\beta_{4}x_{4}+...+ \\beta_{n} + \\epsilon$\n",
    "\n",
    "where, for example,\n",
    "* $\\beta_{1}$ is the temperature coefficient, and $x_{1}$ is the temperature\n",
    "* $\\beta_{2}$ is the occupation coefficient, and $x_{2}$ is the occupation\n",
    "* $\\beta_{3}$ is the time of day coefficient, and $x_{3}$ is the time of day\n",
    "* $\\beta_{n}$ is the intercept (note: no $x$ term)\n",
    "* $\\epsilon$ is the error (should be Gaussian if you identified all of the significant covariates)\n",
    "\n",
    "*Note: Regression is a huge field, this is just a small introduction. We will be doing OLS - Ordinary Least Squares - regression.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do I do regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It depends on the program you use. But the steps are:\n",
    "1. Identify the data and all the covariates (temp, occupation, time of day, etc.)\n",
    "2. Find the regression coefficients for the vector **$\\beta$** (the tricky, program-specific bit)\n",
    "3. Validate your result\n",
    "4. Measure the covariates for the reporting/evaluation period\n",
    "5. Multiply the reporting period data by the coefficients to find the scalar value Y (called the reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Energy use and Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is taken from *Energy Efficiency Measurement and Verification Practices* (2012), Chapter 3, by X. Xia & J. Zhang.\n",
    "\n",
    "The energy use and average monthly maximum temperature at a facility have been given to us in a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline_Temp</th>\n",
       "      <th>Baseline_Energy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>20.5</td>\n",
       "      <td>12625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-28</th>\n",
       "      <td>22.0</td>\n",
       "      <td>12927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-31</th>\n",
       "      <td>19.0</td>\n",
       "      <td>12706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>15.0</td>\n",
       "      <td>12324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-31</th>\n",
       "      <td>13.0</td>\n",
       "      <td>11519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-30</th>\n",
       "      <td>10.5</td>\n",
       "      <td>11116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-31</th>\n",
       "      <td>14.7</td>\n",
       "      <td>11160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-31</th>\n",
       "      <td>20.3</td>\n",
       "      <td>12137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-30</th>\n",
       "      <td>21.3</td>\n",
       "      <td>12585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>21.4</td>\n",
       "      <td>12506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-30</th>\n",
       "      <td>21.0</td>\n",
       "      <td>12726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>22.8</td>\n",
       "      <td>13088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Baseline_Temp  Baseline_Energy\n",
       "Month                                     \n",
       "2014-01-31           20.5            12625\n",
       "2014-01-28           22.0            12927\n",
       "2014-03-31           19.0            12706\n",
       "2014-04-30           15.0            12324\n",
       "2014-05-31           13.0            11519\n",
       "2014-06-30           10.5            11116\n",
       "2014-07-31           14.7            11160\n",
       "2014-08-31           20.3            12137\n",
       "2014-09-30           21.3            12585\n",
       "2014-10-31           21.4            12506\n",
       "2014-11-30           21.0            12726\n",
       "2014-12-31           22.8            13088"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_baseline=pd.read_csv('monthly_baseline.csv',sep='\\;',index_col=0,parse_dates=True)\n",
    "monthly_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the data. It's not necessary for the calculation, but it's always a good idea. We will plot energy use in blue, and temperature in red in a dual-y-axis graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'monthly energy use during baseline period' is not a valid plot kind",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a49be07aaf57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmonthly_baseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseline_Energy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Monthly energy use during baseline period'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmonthly_baseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseline_Temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\u001b[0m\n\u001b[0;32m   3491\u001b[0m                            \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3492\u001b[0m                            \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3493\u001b[1;33m                            **kwds)\n\u001b[0m\u001b[0;32m   3494\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc\u001b[0m in \u001b[0;36mplot_series\u001b[1;34m(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\u001b[0m\n\u001b[0;32m   2581\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m                  \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2583\u001b[1;33m                  **kwds)\n\u001b[0m\u001b[0;32m   2584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_plot_klass\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%r is not a valid plot kind\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'monthly energy use during baseline period' is not a valid plot kind"
     ]
    }
   ],
   "source": [
    "monthly_baseline.Baseline_Energy.plot('Monthly energy use during baseline period')\n",
    "monthly_baseline.Baseline_Temp.plot(secondary_y=True, style='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the energy use decreases as temperature decreases. However, it doesn't give us the relationship between energy use an temperature. For this, we need to do a regression. Assuming a simple linear form, (linear regression doesn't necessary imply straight lines, but in this case we will assume a straight-line relationship):\n",
    "\n",
    "$E = \\alpha T + \\epsilon$\n",
    "\n",
    "where\n",
    "\n",
    "$E$ is the monthly energy use\n",
    "\n",
    "$\\alpha$ is the temperature regression coefficient\n",
    "\n",
    "$T$ is the average monthly maximum temperature\n",
    "\n",
    "$\\epsilon$ is the error\n",
    "\n",
    "We can plot this quite easily using the `seaborn` module, with 90% confidence intervals on the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='Baseline_Temp',y='Baseline_Energy', data=monthly_baseline, ci=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit indicated is the least-squares best fit line, which is the one that minimises the squared errors:\n",
    "\n",
    "**min** $\\sum_{1}^{N}(y_{i}-\\hat{y}_{i})^{2}$\n",
    "\n",
    "This is what OLS-regression means.\n",
    "\n",
    "In order to find the intercept and slope numerically, we can consult the `statsmodels` package, which is the one called in the background by `seaborn` to do the plot above.\n",
    "\n",
    "`Patsy` is another library that we will use to create the matrix/dataframe that we need for the regression. It converts our equation into the matrix, taking categorical variables and such things into account, and translating it into an acceptable `statsmodels` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "energy1, Temp1 = dmatrices('Baseline_Energy ~ Baseline_Temp', data=monthly_baseline, return_type='dataframe')\n",
    "mod = sm.OLS(energy1,Temp1)\n",
    "res = mod.fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there is a two-thirds chance that the slope parameter lies in the interval [123 ; 170] and the intercept lies in the interval [9 132 ; 10 029]. The model can thus be written as:\n",
    "\n",
    "$E_{Adj. Baseline}=146.59\\times T_{Reporting Period}+ 9580.85$\n",
    "\n",
    "Although the goodness of fit metrics above are useful, we need to validate the baseline model by seeing how well it predicts the baseline energy use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_baseline['model_prediction']=monthly_baseline.Baseline_Temp*146.49+9580.85\n",
    "monthly_baseline.Baseline_Energy.plot('Monthly energy use during baseline period',legend=True)\n",
    "monthly_baseline.model_prediction.plot('Model Prediction for baseline',color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be quite close. We note that it underpredicts energy consumption during the first half of the year, and overpredicts it during most of the second half. But we can measure the goodness of fit objectively using the CV(RMSE) and NMBE as described above.  We'll code them as functions to use later. Remember that:\n",
    "\n",
    "$CV(RMSE)=\\frac{\\sqrt{\\frac{1}{N}\\sum_{i}^{N}(y_{i}-\\hat{y}_{i})^{2}}}{\\bar{y}}\\times 100$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CVRMSE (y,y_hat):\n",
    "    return np.sqrt(np.sum((y-y_hat)**2)/len(y))/np.mean(y)*100\n",
    "        \n",
    "CVRMSE_monthly_baseline=CVRMSE(monthly_baseline.Baseline_Energy,monthly_baseline.model_prediction)\n",
    "CVRMSE_monthly_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$NMBE = \\frac{\\frac{1}{N}\\sum_{i}^{N}(y_{i}-\\hat{y}_{i})}{\\bar{y}}\\times 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NMBE(y,y_hat):\n",
    "    return np.sum(y-y_hat)/len(y)/np.mean(y)*100\n",
    "\n",
    "NMBE_monthly_baseline=NMBE(monthly_baseline.Baseline_Energy,monthly_baseline.model_prediction)\n",
    "NMBE_monthly_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definitive ASHRAE 14-2002 Guideline, CV(RMSE) for energy models during the training period should be less than 25%. There is no clear guideline for NMBE, but the models tested by [Lawrence Berkeley shootout cited above](http://eetd.lbl.gov/publications/assessment-of-automated-measurement-a), NMBE's ranged from 0.5 to -4.\n",
    "It should be noted that these values were for models on a 15-minute time scale, not a monthly time scale. Nevertheless, we can be satisfied with the accuracy of our model, given the data at hand.\n",
    "\n",
    "Let us now move to the reporting (post-implementation) period. First we import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_reporting=pd.read_csv('monthly_reporting.csv',sep='\\;',index_col=0,parse_dates=True)\n",
    "monthly_reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our model, we can now predict what the energy use *would have been*, had no changes been made. We call this the 'adjusted baseline':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_reporting['adj_baseline']=monthly_reporting.Reporting_Temp*146.49+9580.85\n",
    "monthly_reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the adjusted baseline and actual reporting period energy use, we find the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_reporting.Reporting_Energy.plot('Reporting period energy use',legend=True,\n",
    "                                        title='Comparison of reporting period energy use to forecast energy use')\n",
    "monthly_reporting.adj_baseline.plot('Adjusted Baseline Energy use', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the reporting period energy use is lower than the adjusted baseline energy use, although the patterns don't match perfectly (this is a real-world example, after all). For what it's worth, we can calculate the energy savings as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_savings = monthly_reporting.adj_baseline.sum(axis=0)-monthly_reporting.Reporting_Energy.sum(axis=0)\n",
    "#print(str(total_savings) + ' kWh')\n",
    "print(\"{:.2f}\".format(total_savings)+ \" kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage savings can be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentage_savings=total_savings/monthly_reporting.Reporting_Energy.sum(axis=0)*100\n",
    "print(\"{:.2f}\".format(percentage_savings) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Buying a Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you buy a car, there are many factors that may influence the price. How do you know that you are getting a good deal? Mileage and year model are two of the significant quantifiable ones you could use to judge this, but there are also certain *categorical* variables, such as the colour and transmission type. The model year can also be seen as a categorical variable rather than a continuous one. We'll consider Suzuki Jimnies as our case study:\n",
    "<img src=\"2015-Suzuki-Jimny.jpg\", style=\"width: 500px\">\n",
    "\n",
    "I've entered the data from 38 Jimnies for sale on [Gumtree](www.gumtree.co.za) during September 2015 into an Excel file, and saved it as Jimnies.csv\n",
    "\n",
    "Notice how we now have continuous variables (mileage) as well as categorical variables (colour, sale location) and ordinal variables (year). This makes our regression more interesting, and we'll need to handle these kinds of variables when we do the load forecasting example.\n",
    "\n",
    "Let's import the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jimnies=pd.read_csv('Jimnies.csv',sep='\\;',index_col=0)\n",
    "jimnies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 'mileage' here refers to kilometers.\n",
    "\n",
    "Let's plot mileage vs. price, just to see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jimnies.plot(kind=\"scatter\", x='Mileage', y='Price')\n",
    "#jimnies.plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that the relationship between mileage and price is definitely not linear in the sense of $y=mx+c$ relationship. However, this does not mean that we can't use linear regression, since the 'linear' in linear regression refers to the linear algebra used to solve the $Y=\\beta X$ equation.\n",
    "\n",
    "The covariates are not continuous variables, and so non-linearity isn't an issue in the same way.\n",
    "\n",
    "Let's see if we can transform the x-axis to give us a straight line. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jimnies['mileage_inv']=jimnies.Mileage**0.5\n",
    "#jimnies.plot(kind=\"scatter\", x='mileage_inv', y='Price')\n",
    "sns.regplot(x='mileage_inv',y='Price', data=jimnies, ci=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Furthermore, you can see that there is a spread on the data. For say, about 40 000 km, the price varies between about R130 000 and R190 000. If we were only regressing price against mileage, this is what we would know. However, we have more information about each of those points, in terms of the colour, model year, and transmission of the unit under consideration. This might be what is forcing certain data points up or down. These would all be dimensions orthogonal on the xy-plane plotted above, and so they are difficult to visualise simultaneously.\n",
    "\n",
    "Now we can fit a regression model to the data by specifying the model in `patsy` and fitting it with `statsmodels`. The model we are specifying may be written in mathematical notation as\n",
    "\n",
    "$Price=$\n",
    "\n",
    "We will fit a regression model to the data using `Python`'s [`statsmodels`](http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/ols.html) library. Most programming languages have similar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price,X = dmatrices('Price ~ mileage_inv + C(Colour) + C(Transmission) + C(Province) + C(jimnies.index)',\n",
    "                    data=jimnies, return_type='dataframe')\n",
    "mod = sm.OLS(price,X)\n",
    "res = mod.fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore the warning for the moment.\n",
    "\n",
    "From the summary statistics above, we can see that we are not at all sure about some of the values for Province and Colour. This should not come as a surprise: there are not many cars available in Green or Red, or in KZN, Mpumalanga, or the Free State. Their $P>|t|$ values (which is a measure of how likely such values would be, given only chance) is close to 100%. In other words they may as well be random, tell us nothing, and may contribute to [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "**Note**: Removing them may decrease the $R^{2}$ value, making it seem as if our model is worse off when it isn't (Read more about it [here](https://en.wikipedia.org/wiki/Coefficient_of_determination)). This is one of the reasons $R^{2}$ isn't one of our metrics of choice. So let us remove the Province and Colour Factors.\n",
    "\n",
    "The real-world significance of those coefficients are that, for the model year, for instance, a 2015 Jimny will cost R9220 more than the \"average\", $\\pm$ R2050, 67% (one standard deviation) of the time.\n",
    "\n",
    "Ok, so let's remove the car colour and sale province from the data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Mileage, Transmission, and Model Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price,X_MTY = dmatrices('Price ~ mileage_inv + C(Transmission) + C(jimnies.index)',\n",
    "                    data=jimnies, return_type='dataframe')\n",
    "mod_MTY = sm.OLS(price,X_MTY)\n",
    "res_MTY = mod_MTY.fit()\n",
    "print res_MTY.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $P>|t|$ values look a bit better, but we seem to still have this warning about a condition number and '[multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)'. What does this mean? (Multi)Colinearity means that our factors are somehow related to one another. Let's do a scatterplot to see what's going on. We'll plot it against `mileage_inv` as that is what the OLS algorithm sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jimnies['year']=jimnies.index\n",
    "jimnies.plot(kind='scatter',x='mileage_inv',y='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there seems to be quite a strong correlation: higher mileage cars are generally older (go figure!).  40 000 - 50 000 km ($200^{2}$) does seem to be a popular mileage to sell a Jimny on, though. But multicollinearity is a problem for our model, because either model year or mileage may be contributing to the high price, and we don't really know which it is.\n",
    "\n",
    "So we have two options: leave both in, including as much information as we can at the risk of overfitting, or take out either the year or the mileage factor. Let's see what happens when we take either one out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering only Transmission and Mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price,X_MT = dmatrices('Price ~ mileage_inv + C(Transmission)',\n",
    "                    data=jimnies, return_type='dataframe')\n",
    "mod_MT = sm.OLS(price,X_MT)\n",
    "res_MT= mod_MT.fit()\n",
    "print res_MT.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering only Transmission and Model Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price,X_TY = dmatrices('Price~ C(Transmission) + C(jimnies.index)',\n",
    "                    data=jimnies, return_type='dataframe')\n",
    "mod_TY = sm.OLS(price,X_TY)\n",
    "res_TY = mod_TY.fit()\n",
    "print res_TY.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we can see that considering mileage seems to be less accurate than considering model year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using $R^{2}$ only, let's investigate CVRMSE and NMBE as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\tCVRMSE \\t\\t R^2 \\t\\tNMBE\")\n",
    "print(\"MTY:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_MTY.predict(X_MTY)))+\"\\t\\t {:.3f}\".format(res_MTY.rsquared)+\"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_MTY.predict(X_MTY))))\n",
    "print(\"MT:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_MT.predict(X_MT)))+\"\\t\\t {:.3f}\".format(res_MT.rsquared)+ \"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_MT.predict(X_MT))))\n",
    "print(\"TY:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_TY.predict(X_TY)))+\"\\t\\t {:.3f}\".format(res_TY.rsquared)+ \"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_TY.predict(X_TY))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model considering Mileage, Transmission, and Year (MTY) is the most accurate, but as we said before, it could be due to overfitting. They way to test this would be to see what happens when we leave certain data points out. This could then give us some idea of what the model would predict for the actual data set (this being the training data set). However, such an investigation would take us too far afield, and so we'll leave it at that for the moment.\n",
    "\n",
    "Note that `statsmodels` makes it really easy to do the predictions. If you have to this manually, it gets quite tedious, because for each data point you have to decide into which of the 8 categories the model year falls, and add/muiltiply the appropriate coefficient. You could also do this with matrix algebra, of course. Most computer languages at your disposal should do this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get to the real example. But before we do, note that this method is taken from [*Quantifying Changes in Building Electricity Use, With Application to Demand Response* by J. L. Mathieu, P. N. Price, S. Kiliccote, M.A. Piette, IEEE Transactions on Smart Grid, Vol. 2, No. 3, September 2011](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5772947&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5772947). The reason for this is explained in [Granderson et al.](http://eetd.lbl.gov/publications/assessment-of-automated-measurement-a), at the beginning of this notebook.\n",
    "\n",
    "In this example, we have the building energy use and temperature data for a given building for 2015. We want to create a load forecasting model so that we can forecast the 2015 energy use, given that we already have 2015's temperature data as well. We actually have the 2015 energy use data as well, but we'll use that at the end to check our forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building2014=pd.read_csv('building2014.csv',sep='\\;',index_col=0,parse_dates=True)\n",
    "building2014.index.names=['Date_Time']\n",
    "building2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are different ways to measure thermal comfort: Average (Temp), considering wind chill, the heat index, the THW index, or the THSW index. We'll keep them for now, so that we can decide on the most accurate one later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building2014.Temp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some missing data in July-August. It shouldn't be too much of a problem, although we will lose some information about the energy-temperature correlation during cold days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-of-Week Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find the 'time of week' for each time period. Since we have 47 time periods per day, and 7 days in a week, we will have 329 time-of-week periods. This is a categorical variable. How you define your time of week period is up to you; in `Pandas` it is easy to get the day-of-week number (the `weekday` property of the dataframe index, which is of the `Datetime` class). We can combine this with the time of day. The format is DayNumber_Time, with Monday being DayNumber=0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unfortunately, this is inelegant:\n",
    "a=pd.Series(building2014.index.time).tolist()\n",
    "b=pd.Series(building2014.index.weekday).tolist()\n",
    "weektime_df=pd.DataFrame(data=pd.Series([\"{}_{}\".format(b,a) for a,b in zip(a, b)]),columns=[\"weektime\"])\n",
    "weektime_df.index=building2014.index\n",
    "building2014['weektime']=weektime_df.weektime\n",
    "building2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't implemented the full model from the article yet, but we can do a regression using only the time-of-week indicator, to see how that fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression_equation='kW ~ weektime*Temp'\n",
    "Energy,X_building = dmatrices(regression_equation, data=building2014, return_type='dataframe')\n",
    "mod_building = sm.OLS(Energy,X_building)\n",
    "res_building= mod_building.fit()\n",
    "#print res_building.summary()\n",
    "y_prediction,X_building2014_predict = dmatrices(regression_equation, data=building2014, return_type='dataframe')\n",
    "energy_prediction=res_building.predict(X_building2014_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now predict the energy use given the temperatures experienced during 2014, to see how well our model fits the known data. \n",
    "\n",
    "Note that what isn't shown here is that lower down in this (long) series, we have missing data.\n",
    "\n",
    "Now we add the forecast as a second column. The thing with `statsmodels 0.6` (the latest version) is that it doesn't handle missing data so well (improvements are promised for 0.7 or 0.8). And we do have missing data. You'll notice that our `energy_forecast` variable is shorter than the actual length of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.size(energy_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.size(building2014.Temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this *may* not be problem in other programming languages, but for `Python`, we now have to put the forecast values in the right places, and leave the right ones blank (or `NaN`: Not a Number). Otherwise our forecast will start lagging from the first time point where we're missing energy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building2014['prediction']='' #create a blank column\n",
    "k=0 #this is the counter for the forecast variable. It doesn't increment if there is no temperature data \n",
    "    #for that time point.\n",
    "for i in range (0,np.size(building2014.Temp)):\n",
    "    if pd.notnull(building2014.Temp[i]) and k<np.size(energy_prediction): #if there is a Temperature reading for that time period, then...\n",
    "        #Note that there is a bit of a hack in the line above, since it bombs out if I dont add the k<... condition.\n",
    "        building2014.prediction[i]=energy_prediction[k] #allocate the right forecast point to that time period\n",
    "        k+=1 #increment forecast counter\n",
    "    else:\n",
    "        building2014.prediction[i]=np.NaN #Skip that time period, write NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a random section of the two, to compare them to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(building2014.kW[10000:11800])\n",
    "plt.plot(building2014.prediction[10000:11800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, huh? It's definitely not perfect, but it's ok. We seem to be consistently underpredicting. I suspect it is because the holidays are dragging us down. But hey, we have some sort of forecast already. Just for the record, let's calculate CVRMSE and NMBE for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"CVRMSE:\\t\" + \"{:.3f}\".format(CVRMSE(building2014.kW,building2014.prediction))+\"\\t NMBE:\" +\"{:.3f}\".format(NMBE(building2014.kW,building2014.prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noting holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to consider public holidays, because otherwise we are introducing a lot of noise into the system. For example, looking at our forecast for the first days of 2015, we find that we overestimate energy use greatly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(building2014.kW[1:1000])\n",
    "plt.plot(building2014.prediction[1:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compensate for this, our model *under*estimates the rest of the year. So we would like to mark public holidays and December holidays, and treat them differently. Note that we don't have to mark weekends, because they are already treated differently in our time-of-week (`weektime`) classification. The 2014 holiday times applicable to the building are:\n",
    "\n",
    "1-13 Jan.\n",
    "\n",
    "12-31 Dec.\n",
    "\n",
    "21 Mar.\n",
    "\n",
    "18, 21, 28 Apr.\n",
    "\n",
    "1, 7 May\n",
    "\n",
    "16 Jun.\n",
    "\n",
    "24 Sept.\n",
    "\n",
    "**2015**:\n",
    "\n",
    "1-12 Jan.\n",
    "\n",
    "3 Apr.\n",
    "\n",
    "6 Apr.\n",
    "\n",
    "27 Apr.\n",
    "\n",
    "1 May\n",
    "\n",
    "16 Jun.\n",
    "\n",
    "I'm going to mark them in the code, but it would be quite easy to just mark them with an extra column in the csv file, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holidays20145 = pd.date_range(start='2014-01-01',end='2014-01-13').union_many([pd.date_range(start='2014-12-12',end='2014-12-31'), \n",
    "['2014-03-21'],['2014-04-18'],['2014-04-21'],['2014-04-28'],['2014-05-01'],['2014-05-01'],['2014-05-07'],\n",
    "['2014-06-16'],['2014-09-24'], pd.date_range(start='2015-01-01',end='2015-01-05'),['2014-04-03'],['2014-04-06'],\n",
    "['2014-04-27'], ['2014-05-01'], ['2014-06-16']])\n",
    "\n",
    "#Create calendar for holidays:\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "workdays_calendar=pd.date_range(start='2014-01-01',end='2015-09-21',freq=CustomBusinessDay(holidays=holidays20145))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is probably a more elegant way to do this in `Python`, but here goes... This ugly piece of code assigns zeroes and ones based on whether it is a holiday (or weekend), or a normal working day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "workdayslist = pd.DataFrame(index=workdays_calendar).index.to_series().apply(lambda x: dt.datetime.strftime(x,'%m-%d-%Y')).tolist()\n",
    "building2014['holiday']=0\n",
    "for i in range(0,np.size(building2014.holiday)):\n",
    "    if not(building2014.index[i].date().strftime('%m-%d-%Y') in workdayslist):\n",
    "            building2014.holiday[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if it worked, let's do a scatterplot of holidays vs workdays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors=np.where(building2014.holiday>0,'b','r')\n",
    "plt.scatter(building2014.Temp,building2014.kW, s=120, c=colors,alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works quite well, but we still have quite a spread on the working day (red) data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After hours energy use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we are still missing some factor: There seem to be two red data series. I suspect that it is because on some hot days, the HVAC system is still switched off after 16:30 and before 07:30 on work days, even though it is cold/hot outside. Let's see if we can catch it. I will use a very similar structure to the holidays one above. \n",
    "\n",
    "The long piece of code just creates a string for a datetime object in a format that `Pandas` understands. Here is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_datetime([building2014.index[1].strftime('%Y')+'-'+building2014.index[1].strftime('%m')+'-'+building2014.index[1].strftime('%d')+ ' 16:30:00'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EES students, please note that you don't need to be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building2014['afterhours']=0\n",
    "for i in range(0,np.size(building2014.afterhours)):\n",
    "    if ((building2014.index[i]>pd.to_datetime([building2014.index[i].strftime('%Y')+'-'+building2014.index[i].strftime('%m')+'-'+building2014.index[i].strftime('%d')+ ' 16:30:00']) \n",
    "        or building2014.index[i]<pd.to_datetime([building2014.index[i].strftime('%Y')+'-'+building2014.index[i].strftime('%m')+'-'+building2014.index[i].strftime('%d')+ ' 07:30:00']))\n",
    "        and building2014.holiday[i]==0):\n",
    "            building2014.afterhours[i]=1\n",
    "colors=np.where(building2014.afterhours>0,'b','r')\n",
    "plt.scatter(building2014.Temp,building2014.kW, s=120, c=colors,alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we have identified the missing factor. We can see three distinct lines: the bottom red one (holidays), the middle blue one (after hours on work days), and the top red one (during work hours on work days).\n",
    "\n",
    "The widely scattered blue dots at the lower end of the temperature range are for the mornings.\n",
    "\n",
    "But now, remember that we classify energy use per `weektime` category? So the after hours thing should be taken into account in our `weektime` variable anyway. No need to add another variable to the dataframe. It's just good that we know what is causing the phenomenon.\n",
    "\n",
    "Now, to be honest, I don't see the piece-wise linear graph described in the paper. I think a straight line fit will be ok, and it is a lot less work, so I'm going to leave it at that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using temperature, time-of-week, and holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to do the regression again, with the holidays added in, but ignoring the after-hours calculation above.\n",
    "\n",
    "A brief note on the `patsy` formula for the regression equation. We need to take *interactions* into account. This means that temperature and time-of-week are not independent (so that we can just + them), but that a hot Wednesday afternoon will be different to a hot Sunday afternoon. Patsy handles these interactions with two operators. Suppose our two factors are a and b:\n",
    "\n",
    ": considers only ab\n",
    "\n",
    "\\* considers a+b+ab\n",
    "\n",
    "The rest of the code below has been copied and pasted from cells above, and should not be new:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression_equation='kW ~ weektime*Temp:holiday'\n",
    "Energy,X_building = dmatrices(regression_equation, data=building2014, return_type='dataframe')\n",
    "mod_building = sm.OLS(Energy,X_building)\n",
    "res_building= mod_building.fit();\n",
    "y_false_forecast,X_building2014 = dmatrices(regression_equation, data=building2014, return_type='dataframe')\n",
    "energy_prediction=res_building.predict(X_building2014)\n",
    "\n",
    "building2014['prediction']='' #create a blank column\n",
    "k=0 #this is the counter for the forecast variable. It doesn't increment if there is no temperature data \n",
    "    #for that time point.\n",
    "for i in range (0,np.size(building2014.Temp)):\n",
    "    if pd.notnull(building2014.Temp[i]) and k<np.size(energy_prediction): #if there is a Temperature reading for that time period, then...\n",
    "        building2014.prediction[i]=energy_prediction[k] #allocate the right forecast point to that time period\n",
    "        k+=1 #increment forecast counter\n",
    "    else:\n",
    "        building2014.prediction[i]=np.NaN #Skip that time period, write NaN.\n",
    "\n",
    "print(\"CVRMSE:\\t\" + \"{:.3f}\".format(CVRMSE(building2014.kW,building2014.prediction))+\"\\t NMBE:\" +\"{:.3f}\".format(NMBE(building2014.kW,building2014.prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can live with a CVRMSE of 14 and a NMBE of 0.001, given the crudeness of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast of 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to forecast the 2015 energy use, given our model with 2014 data. Let's start by importing the Temp2015.csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp2015=pd.read_csv('Temp2015.csv',sep='\\;',index_col=0,parse_dates=True)\n",
    "Temp2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the weektime indicator as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weektime procedure:\n",
    "a=pd.Series(Temp2015.index.time).tolist()\n",
    "b=pd.Series(Temp2015.index.weekday).tolist()\n",
    "weektime_df=pd.DataFrame(data=pd.Series([\"{}_{}\".format(b,a) for a,b in zip(a, b)]),columns=[\"weektime\"])\n",
    "weektime_df.index=Temp2015.index\n",
    "Temp2015['weektime']=weektime_df.weektime\n",
    "Temp2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the 'holidays' procedure on the 2015 data. Remember that I defined the 2015 holidays above when I defined the 2014 holidays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp2015['holiday']=0\n",
    "for i in range(0,np.size(Temp2015.holiday)):\n",
    "    if not(Temp2015.index[i].date().strftime('%m-%d-%Y') in workdayslist):\n",
    "            Temp2015.holiday[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `patsy` needs a `kW` column to do it's thing, we'll create one and give at an arbitrary value of 5. It doesn't really matter, except that if you give it a value of 1, it may think that it is the intercept column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp2015['kW']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed the Temp2015 DataFrame into `patsy` to give us a matrix that `statsmodels` will accept. There are probably better ways of doing this in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_false_forecast,X_building2015 = dmatrices(regression_equation, data=Temp2015, return_type='dataframe')\n",
    "energy_forecast=res_building.predict(X_building2015)\n",
    "energy_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there you have your answer: the 2015 energy use forecast, based on the temperature. However, if you're working in Python, you should remember that this array is shorter - you don't have blank spaces for the periods where you don't have data. So you need to spread it out again to the appropriate time periods, as was done with the 2014 prediction data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Temp2015['forecast']='' #create a blank column\n",
    "k=0 #this is the counter for the forecast variable. It doesn't increment if there is no temperature data \n",
    "    #for that time point.\n",
    "for i in range (0,np.size(Temp2015.Temp)):\n",
    "    if pd.notnull(Temp2015.Temp[i]) and k<np.size(energy_forecast): #if there is a Temperature reading for that time period, then...\n",
    "        Temp2015.forecast[i]=energy_forecast[k] #allocate the right forecast point to that time period\n",
    "        k+=1 #increment forecast counter\n",
    "    else:\n",
    "        Temp2015.forecast[i]=np.NaN #Skip that time period, write NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our forecast to the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the forecast, we can compare it to the 2015 data to see how good it is. Note that in the test you won't have this luxury. The way it is usually done is to 'train' your model on half of your data (or some other percentage), and see how well it forecasts the second half. When you're satisfied, you 'train' it on the full data set to forecast 2015. Or you can use the 2014 and 2015 data sets in this example to test your code.\n",
    "Let's import the 2015 energy values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "energy2015=pd.read_csv('energy2015.csv',sep='\\;',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using the same CVRMSE and NMBE functions defined above to evaluate the fit of them model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"CVRMSE:\\t\" + \"{:.3f}\".format(CVRMSE(energy2015.kW,Temp2015.forecast))+\"\\t NMBE:\" +\"{:.3f}\".format(NMBE(energy2015.kW,Temp2015.forecast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the CVRMSE will be higher than on the 2015 data, but I think that that is pretty cool, given the data at hand and the level of sophistication of the model.\n",
    "In the [Lawrence Berkeley study](http://eetd.lbl.gov/publications/assessment-of-automated-measurement-a) above, commercial software packages were getting CVRMSE's of 7-16, and NMBE's of -3 to -0.5, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(energy2015.kW[8000:10000])\n",
    "plt.plot(Temp2015.forecast[8000:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
